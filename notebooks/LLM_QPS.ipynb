{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time, gc\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODELS = {\n",
    "    \"HQ_Llama\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"Fast_Gemma\": \"google/gemma-2-2b-it\"\n",
    "}\n",
    "BATCH_SIZES = [1, 5]\n",
    "NUM_RUNS = 10  # Number of times to repeat each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    \"\"\"Loads a model and tokenizer, then clears cache to ensure fairness.\"\"\"\n",
    "    \n",
    "    print(f\"\\n--- Loading {model_name} ---\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # CRITICAL FIX: Ensure padding token exists for batching\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    # CRITICAL FIX: Padding side must be LEFT for generation\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        device_map=\"auto\", \n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_model(model, tokenizer, prompt=\"Hello world\"):\n",
    "    \"\"\"Warm up the model to remove first-batch latency spikes.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(**inputs, max_new_tokens=5)\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_batch_performance(model, tokenizer, prompts, max_new_tokens=150):\n",
    "    \"\"\"Measures latency, QPS, and token throughput for a batch.\"\"\"\n",
    "    # Batch tokenization\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    batch_size = len(prompts)\n",
    "    \n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # Count generated tokens per batch\n",
    "    total_tokens = (outputs.shape[1] - inputs.input_ids.shape[1]) * batch_size\n",
    "    \n",
    "    qps = batch_size / total_time\n",
    "    tps = total_tokens / total_time\n",
    "    \n",
    "    return qps, tps, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BENCHMARK ---\n",
    "\n",
    "base_prompts = [\n",
    "    \"Create a podcast intro about AI ethics with a guest scientist.\",\n",
    "    \"Generate a fantasy podcast clip where Gandalf gives life advice.\",\n",
    "    \"Podcast clip about football news with enthusiastic commentary.\",\n",
    "]\n",
    "\n",
    "# Store results\n",
    "results = {model_name: {b: {\"latency\": [], \"qps\": [], \"tps\": []} for b in BATCH_SIZES} \n",
    "           for model_name in MODELS.keys()}\n",
    "\n",
    "for model_name, model_path in MODELS.items():\n",
    "    model, tokenizer = load_model(model_path)\n",
    "\n",
    "    warmup_model(model, tokenizer, prompt=base_prompts[0])\n",
    "    \n",
    "    for b_size in BATCH_SIZES:\n",
    "        # Prepare batch\n",
    "        current_batch = (base_prompts * ((b_size // len(base_prompts)) + 1))[:b_size]\n",
    "        print(current_batch)\n",
    "        for run in range(1, NUM_RUNS + 1):\n",
    "            qps, tps, latency = measure_batch_performance(model, tokenizer, current_batch)\n",
    "            \n",
    "            results[model_name][b_size][\"latency\"].append(latency)\n",
    "            results[model_name][b_size][\"qps\"].append(qps)\n",
    "            results[model_name][b_size][\"tps\"].append(tps)\n",
    "            \n",
    "            print(f\"[{model_name} | Batch {b_size} | Run {run}]\")\n",
    "            print(f\"  Latency: {latency:.2f}s | QPS: {qps:.2f} | TPS: {tps:.2f}\")\n",
    "    \n",
    "    # Cleanup after each model\n",
    "    del model\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Experiment settings:\n",
    "- Hardware: RTX 4090\n",
    "- Max Tokens 150\n",
    "- Warm up yes\n",
    "- Number of Runs 10\n",
    "  \n",
    "\n",
    "# HQ_Llama (8b)\n",
    "\n",
    "### Batch 1:\n",
    "- Latency: 2.98 secs\n",
    "- QPS: 0.34\n",
    "- TPS: 50.43\n",
    "\n",
    "### Batch 5:\n",
    "- Latency: 3.51 secs\n",
    "- QPS: 1.43\n",
    "- TPS: 213\n",
    "\n",
    "# Fast_Gemma (2B)\n",
    "\n",
    "### Batch 1:\n",
    "- Latency: 1.75 secs\n",
    "- QPS: 0.57\n",
    "- TPS: 85\n",
    "\n",
    "### Batch 5:\n",
    "- Latency: 1.90 secs\n",
    "- QPS: 2.7\n",
    "- TPS: 400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
