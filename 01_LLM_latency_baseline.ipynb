{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d83aa223-ed29-465b-b78d-fcc1583f2242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b63e0638-85b2-4841-afa8-114a41e8ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hugging Face API key from environment (do NOT hardcode your token here).\n",
    "import os\n",
    "import logging, warnings\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "# Silence transformers/TRL logs early\n",
    "hf_logging.set_verbosity_error()\n",
    "logging.getLogger(\"trl\").setLevel(logging.ERROR)\n",
    "\n",
    "# Hide specific noisy warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*loss_type=None.*ForCausalLMLoss.*\",\n",
    "    category=UserWarning,\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*cuDNN SDPA backward got grad_output\\.strides\\(\\) != output\\.strides\\(\\).*\",\n",
    "    category=UserWarning,\n",
    ")\n",
    "os.environ[\"TQDM_NOTEBOOK\"] = \"0\"  \n",
    "\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file (if present)\n",
    "load_dotenv()\n",
    "hf_key = os.environ.get(\"HUGGINGFACE_API_KEY\")\n",
    "if hf_key:\n",
    "    login(hf_key)\n",
    "else:\n",
    "    raise EnvironmentError(\"HUGGINGFACE_API_KEY not found. Copy .env.template to .env and add your token. See Instruction.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "597ac528-0049-489f-83c3-cf35bac0f245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:26<00:00,  6.55s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3eca1b8-f9bb-4144-bd36-85e2eb48a5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = \"Create a podcast intro about AI ethics with a guest scientist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bb9920d-5335-4774-bcf9-47847ce2d176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 9.05s\n",
      "Generated Podcast Clip:\n",
      "\n",
      "Create a podcast intro about AI ethics with a guest scientist. Dr. Rachel Kim is a leading expert in AI ethics and will discuss the importance of responsible AI development and deployment. She will share her insights on how to balance the benefits of AI with its potential risks and challenges.\n",
      "\n",
      "Here's a sample podcast intro:\n",
      "\n",
      "**[Upbeat background music starts playing]**\n",
      "\n",
      "Host: Welcome to \"The Future of Tech\", a podcast where we explore the latest advancements in technology and their impact on society. I'm your host, [Name], and today we're talking about AI ethics with a very special guest, Dr. Rachel Kim. Dr. Kim is a renowned expert in AI ethics and has worked with top tech companies to develop responsible AI practices. Welcome to the show, Dr. Kim!\n",
      "\n",
      "**[Music transitions to\n"
     ]
    }
   ],
   "source": [
    "# Inference \n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "output = model.generate(**inputs, max_new_tokens=150)\n",
    "if DEVICE.startswith(\"cuda\"):\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Podcast Clip:\\n\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f9d0dd0-b07a-4b41-87d0-21d184b71de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_8b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:17,  5.71s/it]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.69s/it]\u001b[A\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:05,  5.97s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.04s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gemma_2b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.76s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.69s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"llama_8b\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"gemma_2b\": \"google/gemma-2-2b-it\"\n",
    "}\n",
    "\n",
    "tokenizers = {}\n",
    "llms = {}\n",
    "\n",
    "for name, path in models.items():\n",
    "    print(f\"Loading {name}...\")\n",
    "    tokenizers[name] = AutoTokenizer.from_pretrained(path)\n",
    "    llms[name] = AutoModelForCausalLM.from_pretrained(\n",
    "        path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4e10ea1-5f18-4d15-9991-36e23d835dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_llm_latency(model, tokenizer, prompt, max_tokens=200):\n",
    "    # Warm-up\n",
    "    _ = model.generate(**tokenizer(prompt, return_tensors=\"pt\").to(DEVICE), max_new_tokens=5)\n",
    "    \n",
    "    start = time.time()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    output = model.generate(**inputs, max_new_tokens=max_tokens)\n",
    "    end = time.time()\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    latency = end - start\n",
    "    return latency, generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0f42974-4784-475a-a9b2-8ad522925071",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    # Prompt 1 \n",
    "    \"Create a podcast intro about AI ethics with a guest scientist.\",\n",
    "    # Prompt 2\n",
    "    \"Generate a fantasy podcast clip where Gandalf gives life advice.\",\n",
    "    # Prompt 3\n",
    "    \"Podcast clip about football news with enthusiastic commentary.\",\n",
    "    \n",
    "    # For now we repeat the prompts 1, 2, 3, to test the latency.\n",
    "    \"Create a podcast intro about AI ethics with a guest scientist.\",\n",
    "    \"Generate a fantasy podcast clip where Gandalf gives life advice.\",\n",
    "    \"Podcast clip about football news with enthusiastic commentary.\",\n",
    "\n",
    "    # For now we repeat the prompts 1, 2, 3, to test the latency.\n",
    "    \"Create a podcast intro about AI ethics with a guest scientist.\",\n",
    "    \"Generate a fantasy podcast clip where Gandalf gives life advice.\",\n",
    "    \"Podcast clip about football news with enthusiastic commentary.\",\n",
    "\n",
    "    # For now we repeat the prompts 1, 2, 3 to test the latency.\n",
    "    \"Create a podcast intro about AI ethics with a guest scientist.\",\n",
    "    \"Generate a fantasy podcast clip where Gandalf gives life advice.\",\n",
    "    \"Podcast clip about football news with enthusiastic commentary.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "112a1946-716a-43a5-b95a-c47eb82c3b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_latencies = []\n",
    "gemma_latencies = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    t, _ = measure_llm_latency(llms[\"llama_8b\"], tokenizers[\"llama_8b\"], prompt)\n",
    "    llama_latencies.append(t)\n",
    "    \n",
    "    t, _ = measure_llm_latency(llms[\"gemma_2b\"], tokenizers[\"gemma_2b\"], prompt)\n",
    "    gemma_latencies.append(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e378e8cd-f8cb-4526-9bb1-0e0a574adb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>lat_llama_8b</th>\n",
       "      <th>lat_gemma_2b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Create a podcast intro about AI ethics with a ...</td>\n",
       "      <td>5.968327</td>\n",
       "      <td>21.824368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Generate a fantasy podcast clip where Gandalf ...</td>\n",
       "      <td>5.968938</td>\n",
       "      <td>2.080479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Podcast clip about football news with enthusia...</td>\n",
       "      <td>5.953359</td>\n",
       "      <td>2.083608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Create a podcast intro about AI ethics with a ...</td>\n",
       "      <td>5.948627</td>\n",
       "      <td>2.089503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Generate a fantasy podcast clip where Gandalf ...</td>\n",
       "      <td>5.953940</td>\n",
       "      <td>2.097501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Podcast clip about football news with enthusia...</td>\n",
       "      <td>5.948224</td>\n",
       "      <td>2.100780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Create a podcast intro about AI ethics with a ...</td>\n",
       "      <td>5.945046</td>\n",
       "      <td>2.103452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Generate a fantasy podcast clip where Gandalf ...</td>\n",
       "      <td>5.950995</td>\n",
       "      <td>2.109144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Podcast clip about football news with enthusia...</td>\n",
       "      <td>5.962280</td>\n",
       "      <td>2.108710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Create a podcast intro about AI ethics with a ...</td>\n",
       "      <td>5.950416</td>\n",
       "      <td>2.114704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Generate a fantasy podcast clip where Gandalf ...</td>\n",
       "      <td>5.948015</td>\n",
       "      <td>2.106812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Podcast clip about football news with enthusia...</td>\n",
       "      <td>5.948770</td>\n",
       "      <td>2.145130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               prompt  lat_llama_8b  \\\n",
       "0   Create a podcast intro about AI ethics with a ...      5.968327   \n",
       "1   Generate a fantasy podcast clip where Gandalf ...      5.968938   \n",
       "2   Podcast clip about football news with enthusia...      5.953359   \n",
       "3   Create a podcast intro about AI ethics with a ...      5.948627   \n",
       "4   Generate a fantasy podcast clip where Gandalf ...      5.953940   \n",
       "5   Podcast clip about football news with enthusia...      5.948224   \n",
       "6   Create a podcast intro about AI ethics with a ...      5.945046   \n",
       "7   Generate a fantasy podcast clip where Gandalf ...      5.950995   \n",
       "8   Podcast clip about football news with enthusia...      5.962280   \n",
       "9   Create a podcast intro about AI ethics with a ...      5.950416   \n",
       "10  Generate a fantasy podcast clip where Gandalf ...      5.948015   \n",
       "11  Podcast clip about football news with enthusia...      5.948770   \n",
       "\n",
       "    lat_gemma_2b  \n",
       "0      21.824368  \n",
       "1       2.080479  \n",
       "2       2.083608  \n",
       "3       2.089503  \n",
       "4       2.097501  \n",
       "5       2.100780  \n",
       "6       2.103452  \n",
       "7       2.109144  \n",
       "8       2.108710  \n",
       "9       2.114704  \n",
       "10      2.106812  \n",
       "11      2.145130  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"prompt\": prompts,\n",
    "    \"lat_llama_8b\": llama_latencies,\n",
    "    \"lat_gemma_2b\": gemma_latencies\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00ac79d-adb7-4d02-bf2f-d0c2368d64af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
