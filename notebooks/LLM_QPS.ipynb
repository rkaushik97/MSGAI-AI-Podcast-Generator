{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e67784fa-e1eb-42c6-8ebd-32eb5124d6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 17 19:02:00 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.163.01             Driver Version: 550.163.01     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0 Off |                  Off |\n",
      "| 30%   36C    P5             34W /  450W |       2MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b88876e2-8fe9-41a7-bf2d-489a7b10abf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/homefs/kr23w045/25MSGAI/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, time, gc\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3089e4b7-e782-48d1-9eed-779ae0ef374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODELS = {\n",
    "    \"HQ_Llama\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"Fast_Gemma\": \"google/gemma-2-2b-it\"\n",
    "}\n",
    "BATCH_SIZES = [1, 5]\n",
    "NUM_RUNS = 10  # Number of times to repeat each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2c7d9ae-ba0f-42a9-be57-79ff5acb8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    \"\"\"Loads a model and tokenizer, then clears cache to ensure fairness.\"\"\"\n",
    "    \n",
    "    print(f\"\\n--- Loading {model_name} ---\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # CRITICAL FIX: Ensure padding token exists for batching\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    # CRITICAL FIX: Padding side must be LEFT for generation\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        device_map=\"auto\", \n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54d1456d-71ac-448a-b000-8e7f77a129ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_model(model, tokenizer, prompt=\"Hello world\"):\n",
    "    \"\"\"Warm up the model to remove first-batch latency spikes.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(**inputs, max_new_tokens=5)\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc6b4802-b82e-4311-ac9f-8fe46d0e34b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_batch_performance(model, tokenizer, prompts, max_new_tokens=150):\n",
    "    \"\"\"Measures latency, QPS, and token throughput for a batch.\"\"\"\n",
    "    # Batch tokenization\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    batch_size = len(prompts)\n",
    "    \n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    # Count generated tokens per batch\n",
    "    total_tokens = (outputs.shape[1] - inputs.input_ids.shape[1]) * batch_size\n",
    "    \n",
    "    qps = batch_size / total_time\n",
    "    tps = total_tokens / total_time\n",
    "    \n",
    "    return qps, tps, total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0ff2c53-cb9a-473d-bcef-8385b8d75409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading meta-llama/Llama-3.1-8B-Instruct ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:24<00:00,  6.11s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Create a podcast intro about AI ethics with a guest scientist.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 1 | Run 1]\n",
      "  Latency: 3.05s | QPS: 0.33 | TPS: 49.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 1 | Run 2]\n",
      "  Latency: 2.97s | QPS: 0.34 | TPS: 50.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 1 | Run 3]\n",
      "  Latency: 2.98s | QPS: 0.34 | TPS: 50.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 1 | Run 4]\n",
      "  Latency: 2.97s | QPS: 0.34 | TPS: 50.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 1 | Run 5]\n",
      "  Latency: 2.98s | QPS: 0.34 | TPS: 50.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 1 | Run 6]\n",
      "  Latency: 2.97s | QPS: 0.34 | TPS: 50.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 1 | Run 7]\n",
      "  Latency: 2.98s | QPS: 0.34 | TPS: 50.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 1 | Run 8]\n",
      "  Latency: 2.98s | QPS: 0.34 | TPS: 50.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 1 | Run 9]\n",
      "  Latency: 2.97s | QPS: 0.34 | TPS: 50.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 1 | Run 10]\n",
      "  Latency: 2.97s | QPS: 0.34 | TPS: 50.45\n",
      "['Create a podcast intro about AI ethics with a guest scientist.', 'Generate a fantasy podcast clip where Gandalf gives life advice.', 'Podcast clip about football news with enthusiastic commentary.', 'Create a podcast intro about AI ethics with a guest scientist.', 'Generate a fantasy podcast clip where Gandalf gives life advice.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 5 | Run 1]\n",
      "  Latency: 6.33s | QPS: 0.79 | TPS: 118.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 5 | Run 2]\n",
      "  Latency: 3.49s | QPS: 1.43 | TPS: 215.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 5 | Run 3]\n",
      "  Latency: 3.51s | QPS: 1.43 | TPS: 213.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 5 | Run 4]\n",
      "  Latency: 3.51s | QPS: 1.42 | TPS: 213.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 5 | Run 5]\n",
      "  Latency: 3.51s | QPS: 1.43 | TPS: 213.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 5 | Run 6]\n",
      "  Latency: 3.50s | QPS: 1.43 | TPS: 214.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 5 | Run 7]\n",
      "  Latency: 3.51s | QPS: 1.43 | TPS: 213.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 5 | Run 8]\n",
      "  Latency: 3.51s | QPS: 1.43 | TPS: 213.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HQ_Llama | Batch 5 | Run 9]\n",
      "  Latency: 3.50s | QPS: 1.43 | TPS: 214.11\n",
      "[HQ_Llama | Batch 5 | Run 10]\n",
      "  Latency: 3.70s | QPS: 1.35 | TPS: 202.56\n",
      "\n",
      "--- Loading google/gemma-2-2b-it ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Create a podcast intro about AI ethics with a guest scientist.']\n",
      "[Fast_Gemma | Batch 1 | Run 1]\n",
      "  Latency: 16.55s | QPS: 0.06 | TPS: 9.06\n",
      "[Fast_Gemma | Batch 1 | Run 2]\n",
      "  Latency: 1.91s | QPS: 0.52 | TPS: 78.46\n",
      "[Fast_Gemma | Batch 1 | Run 3]\n",
      "  Latency: 1.75s | QPS: 0.57 | TPS: 85.86\n",
      "[Fast_Gemma | Batch 1 | Run 4]\n",
      "  Latency: 1.75s | QPS: 0.57 | TPS: 85.92\n",
      "[Fast_Gemma | Batch 1 | Run 5]\n",
      "  Latency: 1.77s | QPS: 0.57 | TPS: 84.92\n",
      "[Fast_Gemma | Batch 1 | Run 6]\n",
      "  Latency: 1.81s | QPS: 0.55 | TPS: 83.00\n",
      "[Fast_Gemma | Batch 1 | Run 7]\n",
      "  Latency: 1.75s | QPS: 0.57 | TPS: 85.80\n",
      "[Fast_Gemma | Batch 1 | Run 8]\n",
      "  Latency: 1.75s | QPS: 0.57 | TPS: 85.59\n",
      "[Fast_Gemma | Batch 1 | Run 9]\n",
      "  Latency: 1.68s | QPS: 0.59 | TPS: 89.25\n",
      "[Fast_Gemma | Batch 1 | Run 10]\n",
      "  Latency: 1.73s | QPS: 0.58 | TPS: 86.71\n",
      "['Create a podcast intro about AI ethics with a guest scientist.', 'Generate a fantasy podcast clip where Gandalf gives life advice.', 'Podcast clip about football news with enthusiastic commentary.', 'Create a podcast intro about AI ethics with a guest scientist.', 'Generate a fantasy podcast clip where Gandalf gives life advice.']\n",
      "[Fast_Gemma | Batch 5 | Run 1]\n",
      "  Latency: 27.57s | QPS: 0.18 | TPS: 27.21\n",
      "[Fast_Gemma | Batch 5 | Run 2]\n",
      "  Latency: 1.84s | QPS: 2.72 | TPS: 407.79\n",
      "[Fast_Gemma | Batch 5 | Run 3]\n",
      "  Latency: 1.85s | QPS: 2.70 | TPS: 405.03\n",
      "[Fast_Gemma | Batch 5 | Run 4]\n",
      "  Latency: 1.87s | QPS: 2.68 | TPS: 401.95\n",
      "[Fast_Gemma | Batch 5 | Run 5]\n",
      "  Latency: 1.85s | QPS: 2.70 | TPS: 405.65\n",
      "[Fast_Gemma | Batch 5 | Run 6]\n",
      "  Latency: 1.90s | QPS: 2.63 | TPS: 395.00\n",
      "[Fast_Gemma | Batch 5 | Run 7]\n",
      "  Latency: 1.90s | QPS: 2.64 | TPS: 395.61\n",
      "[Fast_Gemma | Batch 5 | Run 8]\n",
      "  Latency: 1.88s | QPS: 2.66 | TPS: 398.29\n",
      "[Fast_Gemma | Batch 5 | Run 9]\n",
      "  Latency: 1.86s | QPS: 2.68 | TPS: 402.23\n",
      "[Fast_Gemma | Batch 5 | Run 10]\n",
      "  Latency: 1.88s | QPS: 2.67 | TPS: 399.95\n"
     ]
    }
   ],
   "source": [
    "# --- BENCHMARK ---\n",
    "\n",
    "base_prompts = [\n",
    "    \"Create a podcast intro about AI ethics with a guest scientist.\",\n",
    "    \"Generate a fantasy podcast clip where Gandalf gives life advice.\",\n",
    "    \"Podcast clip about football news with enthusiastic commentary.\",\n",
    "]\n",
    "\n",
    "# Store results\n",
    "results = {model_name: {b: {\"latency\": [], \"qps\": [], \"tps\": []} for b in BATCH_SIZES} \n",
    "           for model_name in MODELS.keys()}\n",
    "\n",
    "for model_name, model_path in MODELS.items():\n",
    "    model, tokenizer = load_model(model_path)\n",
    "\n",
    "    warmup_model(model, tokenizer, prompt=base_prompts[0])\n",
    "    \n",
    "    for b_size in BATCH_SIZES:\n",
    "        # Prepare batch\n",
    "        current_batch = (base_prompts * ((b_size // len(base_prompts)) + 1))[:b_size]\n",
    "        print(current_batch)\n",
    "        for run in range(1, NUM_RUNS + 1):\n",
    "            qps, tps, latency = measure_batch_performance(model, tokenizer, current_batch)\n",
    "            \n",
    "            results[model_name][b_size][\"latency\"].append(latency)\n",
    "            results[model_name][b_size][\"qps\"].append(qps)\n",
    "            results[model_name][b_size][\"tps\"].append(tps)\n",
    "            \n",
    "            print(f\"[{model_name} | Batch {b_size} | Run {run}]\")\n",
    "            print(f\"  Latency: {latency:.2f}s | QPS: {qps:.2f} | TPS: {tps:.2f}\")\n",
    "    \n",
    "    # Cleanup after each model\n",
    "    del model\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66adc623-3197-4792-ba8a-1bbd62defbca",
   "metadata": {},
   "source": [
    "# Experiment settings:\n",
    "- Hardware: RTX 4090\n",
    "- Max Tokens 150\n",
    "- Warm up yes\n",
    "- Number of Runs 10\n",
    "  \n",
    "\n",
    "# HQ_Llama (8b)\n",
    "\n",
    "### Batch 1:\n",
    "- Latency: 2.98 secs\n",
    "- QPS: 0.34\n",
    "- TPS: 50.43\n",
    "\n",
    "### Batch 5:\n",
    "- Latency: 3.51 secs\n",
    "- QPS: 1.43\n",
    "- TPS: 213\n",
    "\n",
    "# Fast_Gemma (2B)\n",
    "\n",
    "### Batch 1:\n",
    "- Latency: 1.75 secs\n",
    "- QPS: 0.57\n",
    "- TPS: 85\n",
    "\n",
    "### Batch 5:\n",
    "- Latency: 1.90 secs\n",
    "- QPS: 2.7\n",
    "- TPS: 400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49aa2bf-c9b9-40a4-8375-47674ace4e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
