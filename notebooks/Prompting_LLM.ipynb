{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# LLM Prompting\n",
    "### Author: Kaushik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging, warnings\n",
    "from transformers import logging as hf_logging\n",
    "import transformers\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hugging face token from .env file. \n",
    "\n",
    "# Silence transformers/TRL logs early\n",
    "hf_logging.set_verbosity_error()\n",
    "logging.getLogger(\"trl\").setLevel(logging.ERROR)\n",
    "\n",
    "# Hide specific noisy warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*loss_type=None.*ForCausalLMLoss.*\",\n",
    "    category=UserWarning,\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\".*cuDNN SDPA backward got grad_output\\.strides\\(\\) != output\\.strides\\(\\).*\",\n",
    "    category=UserWarning,\n",
    ")\n",
    "os.environ[\"TQDM_NOTEBOOK\"] = \"0\"  \n",
    "\n",
    "# Load .env file (if present)\n",
    "load_dotenv()\n",
    "hf_key = os.environ.get(\"HUGGINGFACE_API_KEY\")\n",
    "if hf_key:\n",
    "    login(hf_key)\n",
    "else:\n",
    "    raise EnvironmentError(\"HUGGINGFACE_API_KEY not found. Copy .env.template to .env and add your token. See Instruction.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A test cell for selecting a random topic, leave it to the llm to pick the topic and making the podcast.\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Your entire output must start with the exact token: HOST:\\n\\nIf your output contains anything before 'HOST:'—including instructions, explanations, apologies, or any part of this prompt—then your output is invalid.\\n\\nYour entire output must be ONLY a two-person podcast conversation between HOST and GUEST, formatted strictly as:\\nHOST: …\\nGUEST: …\\n(continue dialogue)\\n\\nNo other text, instructions, or blank lines are permitted.\\n\\nContent Requirements\\nPodcast: Adaptive AI Podcast\\nTopic: Pick a interesting topic and \\nGuest: Create a relevant fictional or historical expert name based on the topic\\nLength: 150–200 words\\nSentences short and clean for TTS.\\n\\nDialogue Structure (follow exactly):\\nHOST: hook about topic\\nHOST: podcast + host intro\\nHOST: guest intro\\nGUEST: brief greeting\\nHOST: simple first question\\nGUEST: 3–5 sentence answer\\nHOST: follow-up question\\nGUEST: short answer\\nHOST: closing line: “Let’s get started.”\\n\\n---BEGIN DIALOGUE---\\nHOST:\"\n",
    "    },\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "raw = outputs[0][\"generated_text\"][-1]\n",
    "print(raw['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = outputs[0][\"generated_text\"][-1]\n",
    "print(raw['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify prompt to extract host, guest genders\n",
    "\n",
    "prompt_content = f\"\"\"\n",
    "Your entire output MUST start with the exact token: ---METADATA---\n",
    "\n",
    "---METADATA---\n",
    "HOST_GENDER: [MALE or FEMALE]\n",
    "GUEST_GENDER: [MALE or FEMALE]\n",
    "GUEST_NAME: Create a relevant fictional or historical expert name based on the topic\n",
    "---DIALOGUE---\n",
    "\n",
    "Your entire output must contain a two-person podcast conversation between HOST and GUEST, formatted strictly as:\n",
    "HOST: …\n",
    "GUEST: …\n",
    "(continue dialogue)\n",
    "\n",
    "No other text, instructions, or blank lines are permitted AFTER the '---DIALOGUE---' line.\n",
    "\n",
    "Content Requirements\n",
    "Podcast: Adaptive AI Podcast\n",
    "Topic: The reality of terraforming Mars\n",
    "Length: 150–200 words\n",
    "\n",
    "Dialogue Structure (follow exactly):\n",
    "HOST: hook about topic\n",
    "HOST: podcast + host intro\n",
    "HOST: guest intro (MUST use the GUEST_NAME you generated)\n",
    "GUEST: brief greeting\n",
    "HOST: simple first question\n",
    "GUEST: 3–5 sentence answer\n",
    "HOST: follow-up question\n",
    "GUEST: short answer\n",
    "HOST: closing line: “Let’s get started.”\n",
    "\n",
    "---BEGIN DIALOGUE (must immediately follow the '---DIALOGUE---' line)---\n",
    "HOST:\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt_content\n",
    "    },\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "# Extract the generated content\n",
    "raw = outputs[0][\"generated_text\"][-1]\n",
    "print(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re # Import the regular expression module\n",
    "\n",
    "# Note: The 'pipeline' object/function is assumed to be defined elsewhere \n",
    "# for generating the model output.\n",
    "\n",
    "# List of 50 Topics\n",
    "topics = [\n",
    "    \"The Simulation Theory: Are we living in a video game?\",\n",
    "    \"De-extinction: bringing the Woolly Mammoth back\",\n",
    "    \"The psychology behind why we believe conspiracy theories\",\n",
    "    \"Modern urban legends and their origins\",\n",
    "    \"The future of commercial space tourism\",\n",
    "    \"The enduring mystery of the Bermuda Triangle\",\n",
    "    \"Artificial General Intelligence vs. Human Intuition\",\n",
    "    \"The history of unbreakable codes and ciphers\",\n",
    "    \"Biohacking and the ethics of human augmentation\",\n",
    "    \"The cultural impact of finding alien life\",\n",
    "    \"Deep sea exploration: What is really down there?\",\n",
    "    \"The theoretical physics of time travel\",\n",
    "    \"Minimalist living in a hyper-consumerist world\",\n",
    "    \"The Mandela Effect and collective false memories\",\n",
    "    \"The search for the Lost City of Atlantis\",\n",
    "    \"The science and mystery of lucid dreaming\",\n",
    "    \"The future of lab-grown meat and sustainable food\",\n",
    "    \"Cryptozoology: The hunt for Bigfoot and Nessie\",\n",
    "    \"The Voynich Manuscript: A book no one can read\",\n",
    "    \"Psychological tricks used in modern advertising\",\n",
    "    \"The history and culture of video game speedrunning\",\n",
    "    \"How different languages shape our perception of reality\",\n",
    "    \"The rise of digital nomads and van life\",\n",
    "    \"The Singularity: When machines become smarter than us\",\n",
    "    \"Investigating the reality of paranormal activity\",\n",
    "    \"The Fermi Paradox: Where is everybody?\",\n",
    "    \"The hidden world of the Dark Web\",\n",
    "    \"The Golden Record aboard the Voyager probes\",\n",
    "    \"Ancient civilizations that vanished overnight\",\n",
    "    \"Applying Stoic philosophy to modern stress\",\n",
    "    \"The psychology of cults and charismatic leaders\",\n",
    "    \"Smart Cities: The metropolis of the future\",\n",
    "    \"Sustainable architecture and vertical farming\",\n",
    "    \"The history of espionage and cold war spy gadgets\",\n",
    "    \"Interstellar travel: Wormholes and warp drives\",\n",
    "    \"The weirdest medical treatments in history\",\n",
    "    \"The art of storytelling in the age of TikTok\",\n",
    "    \"Hypothetical survival strategies for a zombie apocalypse\",\n",
    "    \"The mind-bending weirdness of quantum entanglement\",\n",
    "    \"The evolutionary reason why we find things 'cute'\",\n",
    "    \"The secret intelligence of fungi and mushrooms\",\n",
    "    \"Preserving languages on the brink of extinction\",\n",
    "    \"Brain-Computer Interfaces: Merging mind and machine\",\n",
    "    \"The most audacious art heists in history\",\n",
    "    \"The neuroscience of happiness and flow states\",\n",
    "    \"The Great Filter theory of civilization\",\n",
    "    \"The future of privacy in a surveillance state\",\n",
    "    \"A history of unexplained radio signals from space\",\n",
    "    \"The ethics of cloning pets (and people)\",\n",
    "    \"The reality of terraforming Mars\"\n",
    "]\n",
    "\n",
    "podcast_data = []\n",
    "\n",
    "# Parsing Function\n",
    "def parse_metadata_and_dialogue(text):\n",
    "    \"\"\"Parses the gender, name, and dialogue from the model's structured output.\"\"\"\n",
    "    \n",
    "    # Define default values in case parsing fails\n",
    "    metadata = {\n",
    "        \"HOST_GENDER\": \"UNKNOWN\",\n",
    "        \"GUEST_GENDER\": \"UNKNOWN\",\n",
    "        \"GUEST_NAME\": \"Unknown Guest\"\n",
    "    }\n",
    "    dialogue = text\n",
    "\n",
    "    try:\n",
    "        # Split text into metadata block and dialogue block\n",
    "        metadata_block, dialogue_block = text.split(\"---DIALOGUE---\", 1)\n",
    "        \n",
    "        # Clean up the dialogue block (remove leading/trailing whitespace)\n",
    "        dialogue = dialogue_block.strip()\n",
    "\n",
    "        # Use regex to extract metadata key-value pairs from the metadata block\n",
    "        # Pattern looks for a KEY: followed by any characters until the end of the line\n",
    "        pattern = re.compile(r\"(\\w+)_GENDER:\\s*(\\w+)|GUEST_NAME:\\s*(.*)\", re.IGNORECASE)\n",
    "        \n",
    "        for line in metadata_block.split('\\n'):\n",
    "            match = pattern.search(line)\n",
    "            if match:\n",
    "                # Group 1: HOST/GUEST, Group 2: MALE/FEMALE\n",
    "                if match.group(1) and match.group(2): \n",
    "                    key = f\"{match.group(1).upper()}_GENDER\"\n",
    "                    metadata[key] = match.group(2).strip().upper()\n",
    "                # Group 3: GUEST_NAME\n",
    "                elif match.group(3):\n",
    "                    metadata[\"GUEST_NAME\"] = match.group(3).strip()\n",
    "\n",
    "    except ValueError:\n",
    "        # Handle cases where the delimiters are missing\n",
    "        print(\"Warning: Delimiters (---DIALOGUE---) not found. Saving raw output.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during metadata parsing: {e}. Saving raw output.\")\n",
    "        \n",
    "    return metadata, dialogue\n",
    "\n",
    "# Main Generation Loop \n",
    "print(f\"Starting generation for {len(topics)} topics...\")\n",
    "\n",
    "for i, topic in enumerate(topics):\n",
    "    \n",
    "    # The prompt remains the same as it correctly requests the structured output\n",
    "    prompt_content = f\"\"\"\n",
    "Your entire output MUST start with the exact token: ---METADATA---\n",
    "\n",
    "---METADATA---\n",
    "HOST_GENDER: [MALE or FEMALE]\n",
    "GUEST_GENDER: [MALE or FEMALE]\n",
    "GUEST_NAME: Create a relevant fictional or historical expert name based on the topic\n",
    "---DIALOGUE---\n",
    "\n",
    "Your entire output must contain a two-person podcast conversation between HOST and GUEST, formatted strictly as:\n",
    "HOST: …\n",
    "GUEST: …\n",
    "(continue dialogue)\n",
    "\n",
    "No other text, instructions, or blank lines are permitted AFTER the '---DIALOGUE---' line.\n",
    "\n",
    "Content Requirements\n",
    "Podcast: Adaptive AI Podcast\n",
    "Topic: {topic}\n",
    "Length: 150–200 words\n",
    "\n",
    "Dialogue Structure (follow exactly):\n",
    "HOST: hook about topic\n",
    "HOST: podcast + host intro\n",
    "HOST: guest intro (MUST use the GUEST_NAME you generated)\n",
    "GUEST: brief greeting\n",
    "HOST: simple first question\n",
    "GUEST: 3–5 sentence answer\n",
    "HOST: follow-up question\n",
    "GUEST: short answer\n",
    "HOST: closing line: “Let’s get started.”\n",
    "\n",
    "---BEGIN DIALOGUE (must immediately follow the '---DIALOGUE---' line)---\n",
    "HOST:\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_content\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Assuming 'pipeline' is your function call to the language model\n",
    "    # Note: If this is an actual API call, error handling should be included here.\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "\n",
    "    # Extract the generated content\n",
    "    generated_text = outputs[0][\"generated_text\"][-1]['content']\n",
    "    \n",
    "    # Parse the metadata and dialogue\n",
    "    metadata, dialogue_only = parse_metadata_and_dialogue(generated_text)\n",
    "    \n",
    "    entry = {\n",
    "        \"id\": i + 1,\n",
    "        \"topic\": topic,\n",
    "        \"host_gender\": metadata[\"HOST_GENDER\"],\n",
    "        \"guest_gender\": metadata[\"GUEST_GENDER\"],\n",
    "        \"guest_name\": metadata[\"GUEST_NAME\"],\n",
    "        \"dialogue\": dialogue_only\n",
    "    }\n",
    "    \n",
    "    podcast_data.append(entry)\n",
    "    \n",
    "    print(f\"[{i+1}/{len(topics)}] Generated: {topic} | Host: {metadata['HOST_GENDER']}, Guest: {metadata['GUEST_GENDER']} ({metadata['GUEST_NAME']})\")\n",
    "\n",
    "# Save to JSON file\n",
    "output_filename = \"podcast_scripts.json\"\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(podcast_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nSuccessfully saved {len(podcast_data)} conversations to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
