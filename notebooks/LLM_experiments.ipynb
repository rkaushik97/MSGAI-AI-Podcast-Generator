{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVYoKIZm7wlf"
   },
   "source": [
    "## Experiment on LLM model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCzXGT-VT8gA"
   },
   "source": [
    "Setup the git repo and the models, download kokoro files, links in the readme.md and create a folder called kokoro in in the MSGAI-AI-Podcast-Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1234,
     "status": "ok",
     "timestamp": 1765481936389,
     "user": {
      "displayName": "Sebastian KÃ¤slin",
      "userId": "02952943538578971170"
     },
     "user_tz": -60
    },
    "id": "CDJ1mVj0Bv97",
    "outputId": "4e5b2826-fc89-4551-f28d-0dff72c4e9ff"
   },
   "outputs": [],
   "source": [
    "branch_name = \"few-shot-examples\"\n",
    "repo_url = \"https://github.com/rkaushik97/MSGAI-AI-Podcast-Generator.git\"\n",
    "\n",
    "!git clone -b {branch_name} --single-branch {repo_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1360,
     "status": "ok",
     "timestamp": 1765481937752,
     "user": {
      "displayName": "Sebastian KÃ¤slin",
      "userId": "02952943538578971170"
     },
     "user_tz": -60
    },
    "id": "GUWDjuEREK5G"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "# setting key in secrets google colab\n",
    "hf_token = userdata.get('HUGGINGFACE_API_KEY')\n",
    "login(hf_token, add_to_git_credential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1765481937820,
     "user": {
      "displayName": "Sebastian KÃ¤slin",
      "userId": "02952943538578971170"
     },
     "user_tz": -60
    },
    "id": "PYa3moeiEf2l",
    "outputId": "c2d80625-2b6c-4dc0-8b51-960adda3f749"
   },
   "outputs": [],
   "source": [
    "%cd MSGAI-AI-Podcast-Generator/podcast-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1765481937878,
     "user": {
      "displayName": "Sebastian KÃ¤slin",
      "userId": "02952943538578971170"
     },
     "user_tz": -60
    },
    "id": "BOuxC4zTXmxK"
   },
   "outputs": [],
   "source": [
    "# !pip install -U piper-tts kokoro-onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNFg8EkxUjN7"
   },
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530,
     "referenced_widgets": [
      "f4641cdf7c544b2fa3c2fa8ae635c4ab",
      "685470ecafba484db138816b0e5a8ef2",
      "3301ec600d8c4cadbd1e6195364d1c87",
      "2da147d0d8674f59bc3d689e1fffa39a",
      "a776015b883b4443a3ef1e5b73af7b33",
      "ccc76e02f50f476cbb4c49db19437293",
      "8fb73751ec6b4f03a6794698157c5ff6",
      "52c3ed915f0c4669951ce106581a08bc",
      "b64c997ff4ec4d0a8d2ddb51da488a92",
      "9b8f36a42e94463e8bad6d3dd566f5e7",
      "90331df08ec04f03a75f1a3b659f7876",
      "0c494432e62c4c43858aad8392f41a72",
      "8890c2c2c2a84517b9c11ba0c16b01d9",
      "8472c91c148649838fccdd2cc244e4ec",
      "770ecc20e2454e058b0797f864a5cbc4",
      "72ef9f0c450e45339c5d98e1401918ee",
      "85b9eca75ead4e82a82385e411073aa6",
      "93aad61c6a8745e89efe47e2d1b4d079",
      "8a2fcc98bb814a488d9581d8e729963a",
      "25bfbc00ae034fa2adcd5ba9860c91bd",
      "4e2a089859594c449563a2cd2d4afca4",
      "6897d035153d4ce9bb95b47432a25ddb",
      "353e60ceb02e48b08697c6a3f15130bb",
      "4a80c0ec6d5a403197149e53d69892ca",
      "578b787cef9c4b66b99a0cc4124c7e83",
      "9558981dab0e4ad29afaa523d7af4bab",
      "3c7e734f2a9449b998488876ec4d33e4",
      "081b5e13889741d3abfcf3fab08e1186",
      "5fa8bfd320d54080ae52ed58ecf319c7",
      "f960b129b4c5443c94e24a41939c079c",
      "8241cc9dc7f64303bf08c41ba70f8dab",
      "d24eaf78ed76455d9f27939d5e079e87",
      "6c620ba190934c409682cd0fc9272d69",
      "c1680e83708c4ed08f2498e4878f6c87",
      "ca31f5ca410d44d5a4b6d7830c9cfbeb",
      "f5df16e1280c457fa86ef98d37885b77",
      "1db7eaea5ae0463f8d4237fc5fdabbd9",
      "e21ef3e13e1340a49a21231438917c16",
      "46f025763acf4d77acede5e3e53c563b",
      "44508657446e4003a67e4ccb365f11e8",
      "a28b861b71b5408aaa9c698614ebfda8",
      "b3632a0382414ed2a12848dde5e29f0d",
      "bcfb7b2357204930b483bbe0715fcfdf",
      "85cd7c8d2bb941bfa81abfebea165c69",
      "cf7214420bb24373924f10f4bff8f42f",
      "6340af689a624e52b054fba91350572e",
      "ca03a9a9fe784d46a03daa005f55fd34",
      "15366d47288c4d5d93ba0731cffc24f6",
      "d3132edad4f1472c8d3edb82901c3bcc",
      "2386fd44ed294337a0fe55a561252b95",
      "13909c47945e4af4b156f0198edc6ff0",
      "a554441362ea490ba4c1062c926ea0dd",
      "29599e4de65847e68f5b899528eeaf63",
      "679f76e6d69b499fa914136c285e24fb",
      "04cb7dd6a01b4e65917518a07f447af3",
      "1bfe96c6022f461e8cbcc9636d338c2e",
      "506e13a7467b43d3a209c4b0f16803fb",
      "a8dcc6cef6254808a0915ea2f2942cd2",
      "2bf2241c77ed46799ef7bd9c1b5af834",
      "13816d23d4d649239519655725942316",
      "3f5f35b8960a4ae1a48e83686e01fa79",
      "b2e623ff81494fbb8d9327d3221c7eec",
      "0c162845a0eb4ac8a64b5c34700c240f",
      "81b610083ebd495f859999e52e5aaffe",
      "6a3764363a1b4f3cb006f91aa03ad2ab",
      "8a886d85f6f4443da1b513799762031c",
      "fe2da368ecd049b9975716bbea309b0e",
      "47b4b6952b8a4950b56ad81de298266e",
      "619e9d8a60fa49189402c59d1cb60e95",
      "59fe58a6e3ae4de5b9b0c9e9f54de7de",
      "1d24a009881045e7971cf406edd32e16",
      "0ed12f435d4e4b6692aa66c7a8d87cd5",
      "32a816977a5c4f8b81ff6a9ba37bafc0",
      "2103c15c56ae409f92a71e3299fe7205",
      "db3c32b11ac04a64a27171a967497282",
      "d17c8ff537da46588b44e27a274850ea",
      "41e7cc48c6c143aea18282572c66a861",
      "b834f3db6b5142f1b24c40f7ad0db5e9",
      "59c3c2c22b5247988cbf23c224b0d811",
      "f5343f2f69614b198d541743426ee146",
      "fafc7f4a280d4fb2bf413420330a1874",
      "c05d7a9a5ab64ab0bec4d259d751ab99",
      "72b8d4542e4144838ca8c2fb4e360ae1",
      "bc9b6de6ff7342758ad63376d2d4ed18",
      "f5cc8d2850f6476db87811fabe120b12",
      "292953b1d99e475da498ec8ace864c96",
      "bab135236e2c496298388a28f0a5d51b",
      "8a355ad4905d42e294c6c7ee217534ff",
      "26a58fa033a149859898ad16ed715690",
      "dbfab3c6843646bfa0c6d9b9c856b4fe",
      "c175be1c06d5406482540943e071efda",
      "2aea869a375a41b7bef1344ffb9b2b15",
      "9107df8ac6544bc1939f9f1fbfd5ebac",
      "b422dbaca3904fd686d6e7e0f4b5afc8",
      "c60dfedc59ca4cce80c430023be9834e",
      "0a29ab9017de432a8ec63c9c135a2f72",
      "ce6292b719204047a8c36a1a926d8e25",
      "40e99a54610f4f57bd77a516c79439f1",
      "cf6610b178484a66bd604ba5616073b3",
      "b782d313044f419993c836b3b989fac8",
      "0b958e4a0867448abac9955641de5f5e",
      "7204bddb4da94e67a174887901789879",
      "59a53e3ce91d4d569694080ad5952a0d",
      "294546e96eeb4fed857fc943cb6cfc9e",
      "a1316645e5c642dba4643f12b2467fe6",
      "469d28ec379e4d82a12e3ad7040cab7b",
      "b1acfd00badc4c91b2a5104778c1093c",
      "3a69e50f26d34840bda87cd5f4ee3f0a",
      "88277b6287b1419e8b7b9651d6e27e84",
      "f83ef574aad04899852235f29aac8502",
      "235668872937438198ec89370b66ee70",
      "cac31f94488d41b887ef85fb3eda0b91",
      "066947010fdf48c3b89cf72da5ab0bfa",
      "945720089fda4e25b94b3814db446cc7",
      "e5dd5c0f2c77422ba7f38459547dad3f",
      "362e21fdd9eb4be19f552ddccc11ada1",
      "350c0b3bf37248fd8255632c48df7974",
      "f8da20f0af2f4cf9b75f7ad66acafd24",
      "bdedd1b1a6494395892e2061a11e2795",
      "3b54e2c959464b4ca4bba99d715b6eba",
      "1ba804c3bb714b1e94ac6d54d2ba6fc8",
      "33e57a230b51475091373421a7a44da5",
      "87ff1f0fa9f749bab1f6ae6af4522c62",
      "fe7c18409ffc479b9ffad4199a3b3c0e",
      "13264a1afb25419c90c0ef4dd2fcce28",
      "1112fb21c9e34a219031002c6795ac95",
      "794ebbcb68534a07b1d6ebfeb1bf9250",
      "e6f89c37745947f0a669493fde500ef0",
      "37bdaad55bed4172abbff15fad4096ea",
      "a835e9d63bba4ad28fa4f78fbfdb86c4",
      "449bd59290804fe7aca27b4ac16a9e4d",
      "3568eb4c17054849b57bbd79ddd1d7bd"
     ]
    },
    "executionInfo": {
     "elapsed": 58606,
     "status": "ok",
     "timestamp": 1765482513500,
     "user": {
      "displayName": "Sebastian KÃ¤slin",
      "userId": "02952943538578971170"
     },
     "user_tz": -60
    },
    "id": "Ta4s7Pm9Ex92",
    "outputId": "d55097c9-96c9-492a-8200-2d33690fbd0f"
   },
   "outputs": [],
   "source": [
    "from podcast_pipeline.llm_generator import LLMScriptGenerator\n",
    "\n",
    "LLM_MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "prompt_template = \"podcast_script_v1\"\n",
    "\n",
    "llm = LLMScriptGenerator(model_id=LLM_MODEL_ID)\n",
    "# tts = AdaptiveTTSSynthesizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1765482535454,
     "user": {
      "displayName": "Sebastian KÃ¤slin",
      "userId": "02952943538578971170"
     },
     "user_tz": -60
    },
    "id": "MsvtHmhsGsCQ"
   },
   "outputs": [],
   "source": [
    "# a subset of 10 topics taken from topics_batch_1, that are used to test the model quality\n",
    "podcast_topics = [\n",
    "    \"The Simulation Theory: Are we living in a video game?\",\n",
    "    \"De-extinction: bringing the Woolly Mammoth back\",\n",
    "    \"Artificial General Intelligence vs. Human Intuition\",\n",
    "    \"The psychology behind why we believe conspiracy theories\",\n",
    "    \"The future of commercial space tourism\",\n",
    "    \"Theoretical physics of time travel\",\n",
    "    \"Brain-Computer Interfaces: Merging mind and machine\",\n",
    "    \"The Fermi Paradox: Where is everybody?\",\n",
    "    \"The search for the Lost City of Atlantis\",\n",
    "    \"Cryptozoology: The hunt for Bigfoot and Nessie\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1765482816461,
     "user": {
      "displayName": "Sebastian KÃ¤slin",
      "userId": "02952943538578971170"
     },
     "user_tz": -60
    },
    "id": "cGJzY3uYnh9y"
   },
   "outputs": [],
   "source": [
    "experiment_params = {\n",
    "    \"few_shot_examples_nr\": [0,5], #[0, 1, 3, 5],\n",
    "    \"max_token_len\": [256], #[64, 128,256, 512, 1024],\n",
    "    \"temperature\": [0.8],  # encourage originality\n",
    "    \"num_podcasts\": 10,\n",
    "    \"topics\": podcast_topics,\n",
    "    \"cold_start\": [True]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "baf1e5c06ef14f619a6d717eb92fd2ae",
      "ef625ab7bd2a4651b0f6e42881c85536",
      "910a18cd55894ae994d0cf390942d1fe",
      "f39fa696da464fd4a204a8df0ae0b222",
      "e68ea4b1e3ec4d5c8f7afda57ba1cf0c",
      "663df78c48dc40358f1ebdad036e87c2",
      "943eae4199164083a7a5c5434d04fdf6",
      "e19b11476750496296890cc006286f36",
      "73411984fcf946ce98d173e0f8a89516",
      "459e506667484803a96856dff09afc41",
      "b0fdb70118fd41e2b71d8b8cb3ab404c",
      "d881688d02c9474ea6107c3cc0516b02",
      "456570bf78a7421f96a2a3ac306a4b20",
      "48549fd8dd624393861ea385db3f7ab3",
      "862dbd8b5b354a698124670bb46ba88b",
      "31025a9ec7644db3a4d57baa277486dc",
      "8a6db243c4a54b7c8f190fbbc183a360",
      "56f44a5d830d403b8a1e5ba9e3ade2a8",
      "5ec4264302824ecf8ea484ea97dd629b",
      "6f5a8fd44fb148bf864e062600fababb",
      "61481b86dccf47cf9404481f264242da",
      "49126113e5634184aa131da67279b4c4",
      "9318d3f77ee342b2935f1ed65b399537",
      "50215ada6d7d4d19ab1619bb27442bba",
      "b38e0357e3064f988ee31b1f4cfb535b",
      "4f1b586b242f4026b343860ce5d5d45d",
      "012c05717dba482e97830a5f161fd327",
      "f664a448b3c342ffb241cf2e3c7581b2",
      "e1b48d49d14b415199ceae9b8c636cdd",
      "134d45d96e0f42e994e15dccc654fa4b",
      "91740c8255e340e4b3783454c4cbd42a",
      "7a547e1d2748473aa8df9c9a12c7463c",
      "7f040a64dab54a7c98c0f73ffa720e4c",
      "4208645dc4584ae29c35f1f96318a8f7",
      "0821354eab4441208ef2dc2e531ee794",
      "6755d2ec132b4e90b59aec0e58f86065",
      "293aa9c84ebe437cbeb1454b132b3867",
      "66b53097b6d7480abfd7512a3d6d1eef",
      "13f0e6d05d4249e0914b0c5ac54fdd3e",
      "39dfbaea8c604f27aa834812e56b8022",
      "9eb8fda8175e4da8be9aee79d87415e7",
      "b499da7ed2f440c986e44477d2c2e56c",
      "6a18a737b2bc46db8234e434116fb9b9",
      "037cca6aa4c1498599a4d3b074d687ea",
      "d9eb260ed1bc49d3b77cc49737f63c50",
      "370b8144b83049d18b81b972700b78ad",
      "feb1b2ba88134f50ab25eb9784a7a423",
      "75fe94d3a1974292b8187e7501113aef",
      "2847dce2c8ec4f338907aa2908fa8c98",
      "91e2b5eb7ab94a2f8092b4de6a28777b",
      "cd207ebde39a41fea7b8edeefdc8d755",
      "23169832cdb442ea9e154edc01f18717",
      "d34aca4d1581490b98b65a3742b67e68",
      "91c34236480a4c99bbef46b082f2b34a",
      "04fbde96a537413bacb53647da077c82",
      "48c4faff92c24b5d9235f20a334758bf",
      "056b2dbb1eef47baa686e6fb40fe232c",
      "34c5f64944ed44838b80fb66e0b9266a",
      "150cd5e277b54403aa073d6492730588",
      "3c341d1fa24e46d19767ec3ffb80b485",
      "cbd1836c218d4670aee95a8227a01d8d",
      "60ea8d7a930545c3ab8fe582e58dc24c",
      "20aec3a7067e490e8da2188b8dbc2ee5",
      "7192a4017abd4c7d97173ca625dcc02d",
      "2db5c8fe6ac348fa843a5b5ab2a46d23",
      "f75fd3cfdcb342f8a8294e6fb63e76d0",
      "97520727483f43209c5dec66405a5bb7",
      "b2d07071ea5c4b57aa0f62d9a1cb99bc",
      "0c5ce81c15a844749e949ec921cbe36a",
      "036b0e6c40be4c15a8e919782bf2ca79",
      "191bab18ee814d1cb78ddec70e31f939",
      "3ad0a7dec02f4a8e934000814a5c4d08",
      "a9b6ba0f65184dc6a2cb2c62c4c0fc7c",
      "adc03c488f0d4f59baff96985b4d600d",
      "e97a8202914549848039db03788692a7",
      "ed8882cc5c5e447db3bfc1c2ff993b34",
      "c1277055dfb048769e9e0a843f5723fb",
      "f377cb668d4e4e2e8931f0546fa60e93",
      "90577a0db3b54de4aa92395cbf72b505",
      "b8eb55f463f340cbb229276de4057f24",
      "fe3d23f90444458e8aebf847f86fafee",
      "91b99abd933c411e8441c9334663d089",
      "8a66efafcbcd4d8593f088ff9117daca",
      "d3a65c2e12c44235917a0a98164616c1",
      "82d84aea19af4a3ba8d8cf716ee3cca8",
      "58cdd54c3e1b412083c09a1979f83666",
      "500cc55cc46241669850b4f17f8ef163",
      "e1e99a3886964f02b3dc51ad76574234",
      "3d578bff359948a8a06fb310127e4bc8",
      "92adb69957e3495ca293ba85f8054a18",
      "f6252a87dba94dfab03f784511982829",
      "4f57656f146e4074a236ed6fd0cae491",
      "e1cbabfa8d67452ca598213e2ff9c85d",
      "cb9279f00754443cb23924a589c2e96d",
      "d09f6661a68a424c82250ba6ae6c0e8b",
      "2499f1aef1a0488a9a7016b1d37e4b10",
      "44823b7e9f7b4fb6b0361ef802e0f23c",
      "71dc1b3c04ca47a6b79b4bd405235ad8",
      "0c6be6b16cbe4860aca1a74320add9d7",
      "8f6f958ba2f848a1b6fbec2717e15ec6",
      "af90a101b95f4585b7047966aa343ae9",
      "80e47a640c684f63845fe9cb291c9334",
      "76ceace5e01c4c81a029e70dbfb5d733",
      "6f772995d5984a28b4268a812144174a",
      "935aad5b6ad4406ab79edec4ad61e66e",
      "be42715439eb467e9a18d5c8095c31c5",
      "98772f2a225f479583a7531ac06ced1c",
      "84a0231886054c0580519d842aca4daf",
      "f03fd9c8bbfb4cab9b1ce133aa58914f",
      "70fdb28f9e674fbcbd95da423a6cb5af",
      "6b6552a88cd148b295ec94ce81d4e889",
      "770e123143fe43798ca03b58115968a3",
      "a6bae10882264e2d86bdb49b27b1f7d0",
      "4a57c7e864924c668e421e13a5057be4",
      "cb81f6b03ced46788bed1d3bccfed6e2",
      "2bdcc5a664984617a5acd511a1fe0119",
      "3154ba9f21ed4dee8d1792636bd4aca6",
      "53dd41ac259b426fb243f1702d1d19f3",
      "3ce4b74cb639419e8ab65156b21b7a2c",
      "e6605c2315874291b376aec69c4b1078",
      "dac9d2b49ab34c438ae799f68074098c",
      "ffc280243e1d4dd2b5db4b27f66b8091",
      "4ecc659ff8c849bba31168105c34707d",
      "1ce50fb7ca004de0acad748b93af942f",
      "d89b566a266f4fb88793d135bd2e9a1c",
      "98b754485e1b4a81a4af0b8a9b7b0bde",
      "7209ff83677f4263bfd30258764f3c3c",
      "a2ecbe03d09443328675fac9e5a9752a",
      "873b5734074243ccafb5d0537dfb39b6",
      "d3c8ad0e1da5454196ab82dbca641772",
      "6a93753ed80c4350849fb5f976ee4872",
      "212ae352ddc14943b7912810521c20f4",
      "d8752c999b3f4afcbd4a1d565fb3dd62",
      "4a33a50b4a5243fc8e0ab101c890c0ca",
      "f85860382d064c5cac968bbafc3c7417",
      "710157fdd18d4a28ab81e7763f686ec8",
      "6643c342ee7f4bd087e76386df864f4d",
      "ab807e1cdf4e42bdb526d43fdf4f4874",
      "7b9cf63b905542488019074531ed483b",
      "036e19c4302945d1907379dba63fea21",
      "0816a6b3c2e641cb83b65836fa9467bf",
      "be29373cc561440bb8c81283d46bd364",
      "6f0ba70c465f49988a04feb9892264fc"
     ]
    },
    "id": "WuV7lllIq0RA",
    "outputId": "32651275-07b9-456d-e1aa-653233a9bb3a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import json\n",
    "\n",
    "def run_experiment(model_id, prompt_template, topics, params, output_dir):\n",
    "    \"\"\"\n",
    "    Runs inference across topics while measuring generation latency and token counts.\n",
    "    Supports cold-start / warm-start per topic.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Iterate over cold_start modes\n",
    "    for cold in params.get(\"cold_start\", [True]):\n",
    "        print(f\"\\n=== Experiment mode: cold_start={cold} ===\")\n",
    "\n",
    "        # Initialize once if warm start\n",
    "        llm = None\n",
    "        if not cold:\n",
    "            print(\"Warm-start mode: initializing model once and performing dummy inference...\")\n",
    "            llm = LLMScriptGenerator(model_id=model_id)\n",
    "            _ = llm.generate(\n",
    "                \"Warm-up dummy question\",\n",
    "                prompt_template,\n",
    "                max_new_tokens=256,\n",
    "                temperature=0.1\n",
    "            )\n",
    "            print(\"Warm-up completed.\")\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # MAIN LOOP OVER TOPICS\n",
    "        # -----------------------------------------------------------\n",
    "        for topic in topics[: params[\"num_podcasts\"]]:\n",
    "            print(f\"\\nGenerating podcast script for topic: {topic}\")\n",
    "\n",
    "            for few_shot_nr in params[\"few_shot_examples_nr\"]:\n",
    "                for max_len in params[\"max_token_len\"]:\n",
    "                    temperature = params[\"temperature\"][0]\n",
    "\n",
    "                    # ---------------------------------------------------\n",
    "                    # Cold-start handling: reload model every topic\n",
    "                    # ---------------------------------------------------\n",
    "                    if cold:\n",
    "                        print(\" â†’ Cold-start: initializing model...\")\n",
    "                        start_init = time.time()\n",
    "                        llm = LLMScriptGenerator(model_id=model_id)\n",
    "                        end_init = time.time()\n",
    "                        model_load_time = end_init - start_init\n",
    "                    else:\n",
    "                        model_load_time = 0.0  # no reload for warm start\n",
    "\n",
    "                    # ---------------------------------------------------\n",
    "                    # Actual inference\n",
    "                    # ---------------------------------------------------\n",
    "                    start_time = time.time()\n",
    "                    script, _ = llm.generate(\n",
    "                        topic,\n",
    "                        prompt_template,\n",
    "                        few_shot_examples_nr=few_shot_nr,\n",
    "                        max_new_tokens=max_len,\n",
    "                        temperature=temperature\n",
    "                    )\n",
    "                    end_time = time.time()\n",
    "                    inference_time = end_time - start_time\n",
    "\n",
    "                    # ---------------------------------------------------\n",
    "                    # Token stats (optional)\n",
    "                    # ---------------------------------------------------\n",
    "                    try:\n",
    "                        messages = llm._create_prompt(topic, few_shot_examples_nr=few_shot_nr)\n",
    "                        # Concatenate all message contents into a single string\n",
    "                        prompt_text = \"\\n\".join([m[\"content\"] for m in messages])\n",
    "\n",
    "                        input_tokens = llm.count_tokens(prompt_text)\n",
    "\n",
    "                        # Convert metadata to string\n",
    "                        metadata_text = \"\\n\".join([f\"{k}: {v}\" for k, v in script['metadata'].items()])\n",
    "\n",
    "                        # Reconstruct the generated text\n",
    "                        reconstructed_output = f\"---METADATA---\\n{metadata_text}\\n---DIALOGUE---\\n{script['dialogue']}\"\n",
    "\n",
    "                        # Count tokens\n",
    "                        output_tokens = llm.count_tokens(reconstructed_output)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(f\"Error computing tokens for topic: {topic}, few_shot={few_shot_nr}\")\n",
    "                        input_tokens = None\n",
    "                        output_tokens = None\n",
    "\n",
    "                    # ---------------------------------------------------\n",
    "                    # Store results\n",
    "                    # ---------------------------------------------------\n",
    "                    ## Would have been better to save also the original prompt input\n",
    "                    results.append({\n",
    "                        \"topic\": topic,\n",
    "                        \"few_shot_nr\": few_shot_nr,\n",
    "                        \"max_tokens\": max_len,\n",
    "                        \"temperature\": temperature,\n",
    "                        \"cold_start\": cold,\n",
    "                        \"model_load_seconds\": model_load_time,\n",
    "                        \"inference_seconds\": inference_time,\n",
    "                        \"total_time_seconds\": model_load_time + inference_time,\n",
    "                        \"output_text\": script,\n",
    "                        \"prompt_token_count\": input_tokens,\n",
    "                        \"output_token_count\": output_tokens,\n",
    "                        \"total_tokens\": (\n",
    "                            input_tokens + output_tokens\n",
    "                            if input_tokens and output_tokens else None\n",
    "                        )\n",
    "                    })\n",
    "\n",
    "                    print(\n",
    "                        f\"  â†’ few-shot={few_shot_nr}, max_len={max_len}, \"\n",
    "                        f\"cold_start={cold}, load={model_load_time:.2f}s, infer={inference_time:.2f}s\"\n",
    "                    )\n",
    "\n",
    "                    # Cleanup model for next cold start if needed\n",
    "                    if cold:\n",
    "                        del llm\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "    with open(output_dir, 'w') as outfile:\n",
    "      json.dump(results, outfile)\n",
    "    return results\n",
    "\n",
    "results = run_experiment(LLM_MODEL_ID, prompt_template, podcast_topics, experiment_params, 'full_exp.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CawegeiO3Ck"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_llm_gen = time.time()\n",
    "script, scores = llm.generate(podcast_topics[0], prompt_template)\n",
    "end_llm_gen = time.time()\n",
    "\n",
    "# start_tts_gen = time.time()\n",
    "# tts.synthesize(script, 'test.wav')\n",
    "# end_tts_gen = time.time()\n",
    "\n",
    "print(end_llm_gen - start_llm_gen)\n",
    "# print(end_tts_gen - start_tts_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTbfJv6dM5uy"
   },
   "source": [
    "## Few shot examples creation\n",
    "Generate new topics using GPT-5 mini\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LK6l0uM6M4O5"
   },
   "outputs": [],
   "source": [
    "few_shot_examples = [\n",
    "  {\n",
    "    \"topic\": \"The rise of AI-generated art and its impact on creativity\",\n",
    "    \"script\": \"\"\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Forgotten inventions that were ahead of their time\",\n",
    "    \"script\": \"\"\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"The science of dreams: decoding what our subconscious is telling us\",\n",
    "    \"script\": \"\"\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Virtual reality therapy: treating phobias, PTSD, and social anxiety\",\n",
    "    \"script\": \"\"\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"The mysteries of human memory: why we forget and misremember\",\n",
    "    \"script\": \"\"\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Ancient astronomical knowledge and lost observatories\",\n",
    "    \"script\": \"\"\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"The future of deepfake technology and digital identity\",\n",
    "    \"script\": \"\"\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Microbiomes and how gut bacteria influence behavior\",\n",
    "    \"script\": \"\"\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"The untold history of secret societies and hidden power structures\",\n",
    "    \"script\": \"\"\n",
    "  },\n",
    "  {\n",
    "    \"topic\": \"Exploring the psychology of extreme sports and risk-taking behavior\",\n",
    "    \"script\": \"\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DTNe5bXvM-g6"
   },
   "outputs": [],
   "source": [
    "# create the prompt that then I have passed to gpt5-mini\n",
    "# for ex in few_shot_examples:\n",
    "#   ex['prompt'] = llm._create_prompt(ex['topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8t_KIJgax4i"
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('few_shot_examples.json', 'w') as file:\n",
    "#   json.dump(few_shot_examples, file)\n",
    "\n",
    "# with open('few_shot_examples.txt', 'w') as file:\n",
    "#   for ex in few_shot_examples:\n",
    "#     file.write(ex['prompt'][0]['content'])\n",
    "#     file.write('\\n')\n",
    "#     file.write('#'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQmyhcGmnccs"
   },
   "source": [
    "Prompt with few shot examples generated by GPT5-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKNU2WoZidL0"
   },
   "outputs": [],
   "source": [
    "llm._create_prompt(few_shot_examples[0]['topic'], few_shot_examples_nr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wfm1IFH655z"
   },
   "source": [
    "## LLM as judge\n",
    "Model page: https://huggingface.co/meta-llama/Meta-Llama-3-70B\n",
    "\n",
    "âš ï¸ If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/meta-llama/Meta-Llama-3-70B)\n",
    "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) ðŸ™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1765481947477,
     "user": {
      "displayName": "Sebastian KÃ¤slin",
      "userId": "02952943538578971170"
     },
     "user_tz": -60
    },
    "id": "T9a6SSya7i9N"
   },
   "outputs": [],
   "source": [
    "# LLM_JUDGE_ID = \"meta-llama/Meta-Llama-3-70B\" (resources were insufficient)\n",
    "# LLM_JUDGE_ID = \"meta-llama/Meta-Llama-3.1-34B-Instruct\" #(not available)\n",
    "# LLM_JUDGE_ID = \"mistralai/mixtral-8x22b-instruct-v01\" #(not available)\n",
    "\n",
    "LLM_JUDGE_ID = \"mistralai/mistral-7b-instruct-v0.3\"\n",
    "# LLM_JUDGE_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" #(the model itself)\n",
    "\n",
    "\n",
    "LLM_JUDGE_TASK = \"text-generation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 661,
     "referenced_widgets": [
      "a005b10841a64b3fbcb94db00641ecfa",
      "1441c8d798a4413cb2a4f487865e26e1",
      "daa32f346ad14de496ee345d20425b9c",
      "e44c8f28a0fa42dc9a73bf11d5d87c04",
      "e9aa538ec4444ae5a2139e7e1afc0188",
      "7885ed234098463f9440155730112345",
      "a2f1757ab64648c0a409ad429ac25bc0",
      "632355f290b24ded88058b164e716b3f",
      "9c47ba2fe08b44e8b0e19814f4da2781",
      "277cf2c418514385961fe5676ead1b37",
      "ff4712ca6925402da98f75ea5cde2fa4",
      "c9ddc2ef16264c95a0001fcf1a8e5ba9",
      "f1ca0d11288c4b83858bc7cd6a0e8d69",
      "48dd695e2e1c46c18b258f0b3cbe0935",
      "cf58ba1bac9c4fdaadfe26104aa53dd2",
      "b258bfd19a08442c87d7d544ead1738e",
      "692fe6b2b62943898740d08f53325f5e",
      "e0061ba94d40477bbda7565a4ed2acd5",
      "57b9111b54c8459bae72c91df0bc1343",
      "2c2d568577d140838fd5042b141302cc",
      "ea651b5968f1441f9f1d47f73657d4af",
      "2571191d4d634ba8ba0d1ac26a13d2a6",
      "2c479ab5f4074fbba254e401eb706ba3",
      "58a67d45e1d74abcb467ea5e094fe9a9",
      "9c38915f56f74a60a9a9e15e59f1a816",
      "5b5fd7073afc43c6b1f21d1f13d2d4bb",
      "127bdf44149b402a90f801239b282528",
      "3ed5d26355cf4f0186f64494f7aaa77f",
      "8e897330db974c8db3a0f6516806eeb8",
      "c242dcb8449849e499461e020fbe258b",
      "c5e2037bea6748d19e9c12972c6edbc8",
      "c8f7b67a61c341f499f9cf784ad60c09",
      "ef972201349245789c6caf6a660c3052",
      "12cad375ac3c4c14bece4e7ec0f9134b",
      "836f349024e54aeab520809e03fe1ebc",
      "43bc264e05a04ca0b14aede386f22e40",
      "07653a52a6904129a128c73cfb86a7da",
      "d1b0ca3a480a4659bd942e88842c3a71",
      "ac028ca41dbe4987b694e154723bf392",
      "f02e3d87a12743738fa7aefb13ca1c99",
      "455a102414c047c78af3ebafe1db1988",
      "6102947954dd4af6bb3b94845e707a60",
      "6a13ec1cc25d42219383b5617b5b4cb8",
      "3b18865b5ca14e51b3ecda09dfd8549c",
      "25756d3ff180471fbd548931cda24450",
      "2106afd9bdb646e196f0d52c247b6dc0",
      "36ad0ed616cb406ba8ed42614ffda415",
      "7682d8a0a0084f27a59405a5beb7e82b",
      "2385c4b2361c4850b228ed2934360a51",
      "4b08f79970ee46e8afe683fa62d617e1",
      "d1ea40c2db124721a081e847075f3521",
      "4731ee07ed9e45a5a3b1a31a4c3c7a94",
      "1d761be3d5ca469385cf1e74d4dda80a",
      "ab2b66ebc33d4b4aa708c8d4f361973d",
      "626056eb095c425f97fd197dfe10213d",
      "14d9ba345e2a49ecba6b1e5ed5661566",
      "10f6421f89a94e0db459625f3d7510b4",
      "8bb11df5174d41e29f6a65a5cbea0f1f",
      "59029a61c8ea47829ed6cc1e9ea81316",
      "4248f20e48ad433399a68bcb6dc1259a",
      "78556ed65f044f0282c99bc5fc1cdee7",
      "e611bd0cba484a1682f1cedc19d1ed74",
      "5c41041379304472a5429975b1013614",
      "cb23a348ba314d6ea6e36b7bc6abeac7",
      "6d86ae6620314c428660450569c79db9",
      "2b1a4116e73748c8b352ee51a3f72a1f",
      "34c270ef504e49a4be8f43614ad51cce",
      "2483d46998c0451c9f0bb53d72fe88c8",
      "fae9e610d3f2475db64b6e36bfb80c8a",
      "94a7d77855924dc4bf73d3008dfbba7a",
      "a86770ca878745438ea8616721b15ffb",
      "b78c66dd49464f7bbf5d65dcaa081351",
      "8ea842c9c67148c7a0abd80125275178",
      "2710cb99ecf542558c506a6051850b84",
      "3fd8c6f1f2c645648e110d84631a6bf8",
      "80a3b0c66ddb42a9a7014aa37602151e",
      "4a8b97e843f047a0acd02d259d2b22fc",
      "993d4687a955435cac23f39b5bea1a08",
      "b6a0731580714084a946737353311b33",
      "641d79626c2d4bbea708c2e3664ee539",
      "0b66ebecd82d4b5fb15ad284d52f5c43",
      "f3389a3f3fbc48df91f585c241a25e7d",
      "ab7a7353190f4a4893a010f85561da05",
      "d69a7a382cf7482691856d64fb1973d0",
      "bcf908f512c046ac833e476c33b0a4b0",
      "7bac41845f9b4049994b937dff7a91d6",
      "cbb4561b69534f849f46c799c4f73efb",
      "db759ac59f454391846db9fc6072eb1d",
      "ca554a3329614b289f8b8a5f92fa83c1",
      "a593c39cacb34bd597a23e0a7d122010",
      "cbe309bb62904d3d850fe04f0bccdcc0",
      "a6df0ee71c72406b8f6664f448454953",
      "f582a311c05742c8a4dc26f1ea6118b2",
      "fe3c6de236b54507a114784dcd75ea94",
      "20847eaa986d4790bc02919e99332055",
      "50a090cb80344f15b04d203497962064",
      "41dc6507bb374e6689a7debc9a2f88f9",
      "f791b4fdd2ef4c0b9aa27d97ab928d73",
      "9154166bc8244af7b5893171b7da9db6",
      "b9e77373c53741c293fc61bc0431c91e",
      "8626e852f0a24b24926bf6b0ffb4b5b4",
      "9073c232b2744dc58afb8379d6319421",
      "a42f167fda8f45c39795740f7e4229af",
      "4e7aab94f19c432b9c0c2dc2813a2fec",
      "7e53d24b67414eedabda7e7da1f73fcc",
      "53de77c766b3461fbaad7e0757fdbc29",
      "aa15647e2b5c4686898cc29f0620de94",
      "97a099ace69f4fe485eba58273cdda00",
      "841f64a6710f439d8ad6c9fb123a2a84",
      "d44c31920d74484689358ea8e4c9cb9a",
      "17caa0c474454619a7e74b32e86f60be",
      "0d0ffe753a5044d894d6425c5bbfd316",
      "66f260daeacb4fdcacca95516d16911c",
      "c56244dd2e1c4f7ba6e1a180883df520",
      "2fe3cd076a10452787694adabedfa51d",
      "1d52242bcdc44af690e9ac6bf5d5543c",
      "3c766acc601d4bc6a9758ae75e4b65d8",
      "6d5eafed0f4749cc8d6c1ea530e311f1",
      "70a0c111c693473d964b6740ee46da27",
      "195db057e8dc4d3e86b567077884cdc0",
      "34eca40a6fc64b6a98c5b9293e4ccb9a",
      "f5b7904c2cc344deb5d749f75b041fe0",
      "409d58a34369473ea7b77d6dd5b8ba32",
      "ee45a1c6ffed4cadb68d69c32ec5b1ab",
      "c2171cd5cab44ffd8a7e85608b4e939b",
      "fcb3dbfc177a4a83a47a1c3eb20ab5c8",
      "d580a8304bdb4722b5afe605de6745c4",
      "9b1d2afc0c3d4c748f3100a6d9bc4834",
      "6726c212542844f7acc941a5ba62b5ee",
      "e3ad9faa9db24854a71cda4d77b22ea1",
      "0165dbf5a7024dbabffd0e2e323604a8",
      "e86540a66b494222b066f2122750599b"
     ]
    },
    "executionInfo": {
     "elapsed": 74391,
     "status": "ok",
     "timestamp": 1765482024152,
     "user": {
      "displayName": "Sebastian KÃ¤slin",
      "userId": "02952943538578971170"
     },
     "user_tz": -60
    },
    "id": "iVKvAKiS7i9P",
    "outputId": "081bd515-14d4-4e9a-b928-cecc92424e1e"
   },
   "outputs": [],
   "source": [
    "from podcast_pipeline.llm_generator import LLMScriptGenerator\n",
    "\n",
    "llm_judge = LLMScriptGenerator(model_id=LLM_JUDGE_ID, model_task=LLM_JUDGE_TASK)\n",
    "# llm_judge = LLMScriptGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E4U8_JvTHX10"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from podcast_pipeline.llm_generator import LLMScriptGenerator\n",
    "\n",
    "def evaluate_result(result, model_id, template_key):\n",
    "  # reconstruct the original prompt\n",
    "  original_prompt = llm_judge._create_prompt(result['topic'])[0]['content']\n",
    "\n",
    "  # topic of the request and generated script {topic: , metadata: , dialogue: }\n",
    "  original_topic = result['topic']\n",
    "  generated_script = result[\"output_text\"]\n",
    "\n",
    "  # reconstruct the string of the original output\n",
    "  output = LLMScriptGenerator.reconstruct_llm_output(generated_script['metadata'], generated_script['dialogue'])\n",
    "\n",
    "  # format the prompt for the judge (v2 include also the original prompt -> compliance score)\n",
    "  system_prompt, user_query = llm_judge._prompt_manager.format_judge_prompt(\n",
    "                  template_key,\n",
    "                  original_topic=original_topic,\n",
    "                  generated_script=output,\n",
    "                  original_prompt=original_prompt\n",
    "              )\n",
    "\n",
    "  if model_id not in ['meta-llama/Meta-Llama-3.1-8B-Instruct','mistralai/mistral-7b-instruct-v0.3']:\n",
    "    print(\"The model is not integrated, set input prompt creation and output extraction :)\")\n",
    "    return None\n",
    "  elif model_id == 'mistralai/mistral-7b-instruct-v0.3':\n",
    "    # mixtral formatting\n",
    "    prompt_input = (\n",
    "        \"<s>[INST] \"\n",
    "        f\"{system_prompt}\\n\\n{user_query} \"\n",
    "        \"[/INST] \"\n",
    "    )\n",
    "  elif model_id == 'meta-llama/Meta-Llama-3.1-8B-Instruct':\n",
    "    # prompt formatted as a conversation (8b LLAMA)\n",
    "    prompt_input = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_query}\n",
    "    ]\n",
    "\n",
    "\n",
    "  judge_output = llm_judge._pipeline(\n",
    "      prompt_input,\n",
    "      max_new_tokens=256,\n",
    "      do_sample=False,\n",
    "      temperature=0.1,\n",
    "      pad_token_id=llm_judge._pipeline.tokenizer.eos_token_id,\n",
    "  )\n",
    "\n",
    "  if model_id == 'mistralai/mistral-7b-instruct-v0.3':\n",
    "    # Extract generated text\n",
    "    judge_text = judge_output[0][\"generated_text\"]\n",
    "\n",
    "  elif model_id == 'meta-llama/Meta-Llama-3.1-8B-Instruct':\n",
    "    # Extract the generated content (assuming it's the last message content)\n",
    "    judge_text = judge_output[0][\"generated_text\"][-1]['content']\n",
    "\n",
    "  else:\n",
    "    print(f'Define how the the output should be extracted for {model_id}')\n",
    "    return None\n",
    "\n",
    "  # # Attempt to extract JSON from the output\n",
    "  json_match = re.search(r\"\\{.*\\}\", judge_text, re.DOTALL)\n",
    "  if json_match:\n",
    "      evaluation = json.loads(json_match.group(0))\n",
    "  else:\n",
    "      # fallback if JSON not found\n",
    "      evaluation = {\"relevance_score\": None, \"coherence_score\": None, \"compliance_score\": None}\n",
    "\n",
    "  return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3qV9vFm9SoJ"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# load exp results\n",
    "results_path = '../notebooks/llm_judge/full_exp_no_scores.json'\n",
    "# results_path = 'full_exp.json'\n",
    "with open(results_path) as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# template_judge_key = \"llm_judge_v2\" #(granularity 1-5)\n",
    "template_judge_key = \"llm_judge_v3\" #(granularity 1-10)\n",
    "\n",
    "# llama 70b expect a chat template\n",
    "for entry in tqdm(results):\n",
    "    judge_output = evaluate_result(entry, LLM_JUDGE_ID, template_judge_key)\n",
    "    entry['relevance_score'] = judge_output.get(\"relevance_score\", None)\n",
    "    entry[\"coherence_score\"] = judge_output.get(\"coherence_score\", None)\n",
    "    entry[\"compliance_score\"] = judge_output.get(\"compliance_score\", None)\n",
    "\n",
    "# Optionally save back to JSON\n",
    "with open(\"results_with_scores.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_T4RGnq6Nu1"
   },
   "source": [
    "## Test Judge on few shot examples (originals and with errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1765482024168,
     "user": {
      "displayName": "Sebastian KÃ¤slin",
      "userId": "02952943538578971170"
     },
     "user_tz": -60
    },
    "id": "bcPWw1iftbaP"
   },
   "outputs": [],
   "source": [
    "def evaluate_few_shot_example(few_shot_example, model_id, template_key):\n",
    "  # reconstruct the original prompt\n",
    "  original_prompt = few_shot_example['prompt'][0]['content']\n",
    "\n",
    "  # topic of the request and generated script {topic: , metadata: , dialogue: }\n",
    "  original_topic = few_shot_example['topic']\n",
    "  output = few_shot_example[\"script\"]\n",
    "\n",
    "  # format the prompt for the judge (v2 include also the original prompt -> compliance score)\n",
    "  system_prompt, user_query = llm_judge._prompt_manager.format_judge_prompt(\n",
    "                  template_key,\n",
    "                  original_topic=original_topic,\n",
    "                  generated_script=output,\n",
    "                  original_prompt=original_prompt\n",
    "              )\n",
    "\n",
    "  if model_id not in ['meta-llama/Meta-Llama-3.1-8B-Instruct','mistralai/mistral-7b-instruct-v0.3']:\n",
    "    print(\"The model is not integrated, set input prompt creation and output extraction :)\")\n",
    "    return None\n",
    "  elif model_id == 'mistralai/mistral-7b-instruct-v0.3':\n",
    "    # mixtral formatting\n",
    "    prompt_input = (\n",
    "        \"<s>[INST] \"\n",
    "        f\"{system_prompt}\\n\\n{user_query} \"\n",
    "        \"[/INST] \"\n",
    "    )\n",
    "  elif model_id == 'meta-llama/Meta-Llama-3.1-8B-Instruct':\n",
    "    # prompt formatted as a conversation (8b LLAMA)\n",
    "    prompt_input = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_query}\n",
    "    ]\n",
    "\n",
    "  judge_output = llm_judge._pipeline(\n",
    "      prompt_input,\n",
    "      max_new_tokens=256,\n",
    "      do_sample=False,\n",
    "      temperature=0.1,\n",
    "      pad_token_id=llm_judge._pipeline.tokenizer.eos_token_id,\n",
    "  )\n",
    "\n",
    "  if model_id == 'mistralai/mistral-7b-instruct-v0.3':\n",
    "    # Extract generated text\n",
    "    judge_text = judge_output[0][\"generated_text\"]\n",
    "\n",
    "  elif model_id == 'meta-llama/Meta-Llama-3.1-8B-Instruct':\n",
    "    # Extract the generated content (assuming it's the last message content)\n",
    "    judge_text = judge_output[0][\"generated_text\"][-1]['content']\n",
    "\n",
    "  else:\n",
    "    print(f'Define how the the output should be extracted for {model_id}')\n",
    "    return None\n",
    "\n",
    "  # # Attempt to extract JSON from the output\n",
    "  json_match = re.search(r\"\\{.*\\}\", judge_text, re.DOTALL)\n",
    "  if json_match:\n",
    "      evaluation = json.loads(json_match.group(0))\n",
    "  else:\n",
    "      # fallback if JSON not found\n",
    "      evaluation = {\"relevance_score\": None, \"coherence_score\": None, \"compliance_score\": None}\n",
    "\n",
    "  return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15987,
     "status": "ok",
     "timestamp": 1765482092428,
     "user": {
      "displayName": "Sebastian KÃ¤slin",
      "userId": "02952943538578971170"
     },
     "user_tz": -60
    },
    "id": "XADDsCyRvK9R",
    "outputId": "26893b8c-335a-4f21-8101-637833ec38b1"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# load exp results\n",
    "# few_shot_examples_path = 'input/few_shot_examples_responses.json'\n",
    "few_shot_examples_path = 'input/few_shot_examples_responses_with_errors.json'\n",
    "\n",
    "with open(few_shot_examples_path) as f:\n",
    "    few_shot_examples = json.load(f)\n",
    "\n",
    "# template_judge_key = \"llm_judge_v2\" #(granularity 1-5)\n",
    "template_judge_key = \"llm_judge_v3\" #(granularity 1-10)\n",
    "\n",
    "# llama 70b expect a chat template\n",
    "for entry in tqdm(few_shot_examples):\n",
    "    judge_output = evaluate_few_shot_example(entry, LLM_JUDGE_ID, template_judge_key)\n",
    "    entry['relevance_score'] = judge_output.get(\"relevance_score\", None)\n",
    "    entry[\"coherence_score\"] = judge_output.get(\"coherence_score\", None)\n",
    "    entry[\"compliance_score\"] = judge_output.get(\"compliance_score\", None)\n",
    "\n",
    "# Optionally save back to JSON\n",
    "with open(\"few_shot_examples_with_scores.json\", \"w\") as f:\n",
    "    json.dump(few_shot_examples, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Qfik0CV7RN3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO2vbxXSix3RcZXg0yocntf",
   "collapsed_sections": [
    "lTbfJv6dM5uy",
    "_wfm1IFH655z",
    "w_T4RGnq6Nu1"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
